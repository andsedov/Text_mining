{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Analysis\n",
    "This is a decision support system which classifies important sentences in technical documents in Russian and filters unimportant. Manually labeled sentences from several documents were used for the training set.\n",
    "\n",
    "Training was performed with Naive Bayes classification algorithm with stratified 10-fold cross-validation.\n",
    "Feature extraction from text was performed with Tf-Idf vectorizer from NLTK package. Demencionality of vocabulary was reduced with PCA to decrease computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "exclude = set(string.punctuation)\n",
    "exclude.remove('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train.txt') as f:\n",
    "    text = f.read().splitlines()\n",
    "    \n",
    "def stripper(s):\n",
    "    s = ''.join(ch for ch in s if ch not in exclude)\n",
    "    s = re.sub(\"[\\t\\n№\\d«»–]'\", ' ', s)\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "def strippers(s):\n",
    "    s = ''.join(ch for ch in s if ch not in exclude)\n",
    "    s = re.sub('[\\t№\\n\\d«»–]', ' ', s)\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "train = []\n",
    "for line in text:\n",
    "    train.append(stripper(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('target.txt') as f:\n",
    "    target = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(train) == len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = []\n",
    "for label in target:\n",
    "    Y.append(int(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction and training\n",
    "Every sentence is a bag-of-words representation with stemming and reduced stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8')\n",
    "X = vectorizer.fit_transform(train)\n",
    "X = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2364, 1436)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77979769671\n",
      "Accuracy: 83.76% (6.40%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Naive Bayes ###\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "pca = decomposition.PCA(n_components=180)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "\n",
    "clf = naive_bayes.GaussianNB()\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(clf, X_pca, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "clf.fit(X_pca, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression was used as a baseline for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Logistic Regression ###\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
    "\n",
    "# logit = LogisticRegression()\n",
    "# logit_params = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# grid = GridSearchCV(estimator=logit, param_grid=logit_params, n_jobs=-1, cv=skf, verbose=1, scoring='accuracy')\n",
    "\n",
    "# grid_result = grid.fit(X_pca, Y)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Prediction is based on the probability of sentence being helpful or not. The function defined below requires a probability threshold. Low theshold decrease the algorithm accuracy but reduce the number of false negative cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pprint\n",
    "\n",
    "with open('~/home/docs.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "list_of_documents = list(data.keys())\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stoplist = get_stop_words('russian')\n",
    "stemmer = SnowballStemmer('russian')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#document_name = list_of_documents[15]\n",
    "\n",
    "def predict_importance(document_name, threshold = 0.3):\n",
    "    \n",
    "    print('Document Name: ', document_name)\n",
    "    print('====================================')\n",
    "    doc = data[str(document_name)]['content']\n",
    "    sentences = strippers(doc).split('. ')\n",
    "\n",
    "    for sentence in sentences:\n",
    "        initial_sentence = sentence\n",
    "        sentence = [word for word in sentence.lower().split() if word not in stoplist]\n",
    "        for i, word in enumerate(sentence):\n",
    "                sentence[i] = stemmer.stem(word)\n",
    "        if sentence != []:\n",
    "            sentence = ' '.join(sentence)\n",
    "\n",
    "            ############ Validation ###########\n",
    "\n",
    "            vector = vectorizer.transform([sentence])\n",
    "            is_important = clf.predict(pca.transform(vector.todense()))\n",
    "            probability = clf.predict_proba(pca.transform(vector.todense()))\n",
    "            if probability[0][1] > threshold:\n",
    "                print(initial_sentence, probability[0][1])\n",
    "                print('-----------------------------------------')\n",
    "    print('========== Document End ===========')\n",
    "\n",
    "predict_importance(document_name)   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
